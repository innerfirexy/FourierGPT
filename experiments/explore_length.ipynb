{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from run_fft import FFTProcessor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable rpy2\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the lengths of Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_processor = FFTProcessor(method='fft', \n",
    "                             preprocess='logzs', \n",
    "                             value='norm', \n",
    "                             require_sid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing human lengths: 203.25333333333333 19.312581966744432\n",
      "writing model lengths: 207.29333333333332 15.295989307295194\n"
     ]
    }
   ],
   "source": [
    "est_name = 'gpt2xl'\n",
    "\n",
    "nll_xsum_orig = fft_processor._read_data(data_file=f'../data/gpt-4/writing_gpt-4.original.{est_name}.nll.txt')\n",
    "nll_xsum_samp = fft_processor._read_data(data_file=f'../data/gpt-4/writing_gpt-4.sampled.{est_name}.nll.txt')\n",
    "\n",
    "print('writing human lengths:', \n",
    "      np.mean(list(map(len, nll_xsum_orig))),\n",
    "      np.std(list(map(len, nll_xsum_orig))))\n",
    "print('writing model lengths:',\n",
    "      np.mean(list(map(len, nll_xsum_samp))),\n",
    "      np.std(list(map(len, nll_xsum_samp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nlls(nlls, output_file):\n",
    "    import torch\n",
    "    with open(output_file, 'w') as f:\n",
    "        for res in nlls:\n",
    "            if isinstance(res, torch.Tensor):\n",
    "                res = res.numpy().tolist()\n",
    "            res_str = ' '.join(f'{num:.4f}' for num in res)\n",
    "            f.write(f'{res_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop the first k=50, 100, 150 tokens\n",
    "est_name = 'gpt2xl'\n",
    "chop_k = 150\n",
    "\n",
    "nll_xsum_orig_chop = [nll[:chop_k] for nll in nll_xsum_orig]\n",
    "nll_xsum_samp_chop = [nll[:chop_k] for nll in nll_xsum_samp]\n",
    "\n",
    "write_nlls(nll_xsum_orig_chop, f'../data/short/writing_gpt-4.original.{est_name}.chop{chop_k}.nll.txt')\n",
    "write_nlls(nll_xsum_samp_chop, f'../data/short/writing_gpt-4.sampled.{est_name}.chop{chop_k}.nll.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "require(\"data.table\")\n",
    "require(\"ggplot2\")\n",
    "\n",
    "orig_chop50 <- fread(\"../data/short/writing_gpt-4.original.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "orig_chop50$Group <- \"Human\"\n",
    "samp_chop50 <- fread(\"../data/short/writing_gpt-4.sampled.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "samp_chop50$Group <- \"Model\"\n",
    "\n",
    "orig_chop100 <- fread(\"../data/short/writing_gpt-4.original.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "orig_chop100$Group <- \"Human\"\n",
    "samp_chop100 <- fread(\"../data/short/writing_gpt-4.sampled.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "samp_chop100$Group <- \"Model\"\n",
    "\n",
    "orig_chop150 <- fread(\"../data/short/writing_gpt-4.original.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "orig_chop150$Group <- \"Human\"\n",
    "samp_chop150 <- fread(\"../data/short/writing_gpt-4.sampled.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "samp_chop150$Group <- \"Model\"\n",
    "\n",
    "orig_full <- fread(\"../data/gpt-4/writing_gpt-4.original.gpt2xl.nllzs.fftnorm.txt\")\n",
    "orig_full$Group <- \"Human\"\n",
    "samp_full <- fread(\"../data/gpt-4/writing_gpt-4.sampled.gpt2xl.nllzs.fftnorm.txt\")\n",
    "samp_full$Group <- \"Model\"\n",
    "\n",
    "d_chop50 <- rbind(samp_chop50, orig_chop50)\n",
    "d_chop50$ChopK <- \"50\"\n",
    "d_chop100 <- rbind(samp_chop100, orig_chop100)\n",
    "d_chop100$ChopK <- \"100\"\n",
    "d_chop150 <- rbind(samp_chop150, orig_chop150)\n",
    "d_chop150$ChopK <- \"150\"\n",
    "d_full <- rbind(samp_full, orig_full)\n",
    "d_full$ChopK <- \"Full\"\n",
    "d_chop <- rbind(d_chop50, d_chop100, d_chop150, d_full)\n",
    "d_chop$ChopK <- factor(d_chop$ChopK, levels=c(\"50\", \"100\", \"150\", \"Full\"))\n",
    "\n",
    "p <- ggplot(d_chop, aes(x=freq, y=power)) + \n",
    "    geom_smooth(aes(fill=Group, colour=Group, linetype=Group)) + \n",
    "    theme_bw() + theme(legend.position=c(.9,.2)) +\n",
    "    scale_color_brewer(palette=\"Set1\") + scale_fill_brewer(palette=\"Set1\") +\n",
    "    labs(x = bquote(omega[k]), y = bquote(X(omega[k]))) + \n",
    "    facet_wrap(~ChopK, ncol=4)\n",
    "ggsave(\"writing_chop50_100_150.pdf\", plot=p, width=9, height=3)\n",
    "# plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chop xsum by lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum human lengths: 210.03333333333333 18.089929672487827\n",
      "xsum model lengths: 205.53333333333333 14.614908674211945\n"
     ]
    }
   ],
   "source": [
    "est_name = 'gpt2xl'\n",
    "\n",
    "nll_xsum_orig = fft_processor._read_data(data_file=f'../data/gpt-4/xsum_gpt-4.original.{est_name}.nll.txt')\n",
    "nll_xsum_samp = fft_processor._read_data(data_file=f'../data/gpt-4/xsum_gpt-4.sampled.{est_name}.nll.txt')\n",
    "\n",
    "print('xsum human lengths:', \n",
    "      np.mean(list(map(len, nll_xsum_orig))),\n",
    "      np.std(list(map(len, nll_xsum_orig))))\n",
    "print('xsum model lengths:',\n",
    "      np.mean(list(map(len, nll_xsum_samp))),\n",
    "      np.std(list(map(len, nll_xsum_samp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop the first k=50, 100, 150 tokens\n",
    "est_name = 'gpt2xl'\n",
    "chop_k = 100\n",
    "\n",
    "nll_xsum_orig_chop = [nll[:chop_k] for nll in nll_xsum_orig]\n",
    "nll_xsum_samp_chop = [nll[:chop_k] for nll in nll_xsum_samp]\n",
    "\n",
    "write_nlls(nll_xsum_orig_chop, f'../data/short/xsum_gpt-4.original.{est_name}.chop{chop_k}.nll.txt')\n",
    "write_nlls(nll_xsum_samp_chop, f'../data/short/xsum_gpt-4.sampled.{est_name}.chop{chop_k}.nll.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs path for data: /Users/xy/projects/FourierGPT/data/short\n",
      "abs path for script: /Users/xy/projects/FourierGPT\n",
      "python /Users/xy/projects/FourierGPT/run_fft.py -i /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.original.gpt2xl.chop50.nll.txt -o /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.original.gpt2xl.chop50.nllzs.fftnorm.txt -p zscore --value norm\n",
      "python /Users/xy/projects/FourierGPT/run_fft.py -i /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.original.gpt2xl.chop100.nll.txt -o /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.original.gpt2xl.chop100.nllzs.fftnorm.txt -p zscore --value norm\n",
      "python /Users/xy/projects/FourierGPT/run_fft.py -i /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.original.gpt2xl.chop150.nll.txt -o /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.original.gpt2xl.chop150.nllzs.fftnorm.txt -p zscore --value norm\n",
      "python /Users/xy/projects/FourierGPT/run_fft.py -i /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.sampled.gpt2xl.chop50.nll.txt -o /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.sampled.gpt2xl.chop50.nllzs.fftnorm.txt -p zscore --value norm\n",
      "python /Users/xy/projects/FourierGPT/run_fft.py -i /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.sampled.gpt2xl.chop100.nll.txt -o /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.sampled.gpt2xl.chop100.nllzs.fftnorm.txt -p zscore --value norm\n",
      "python /Users/xy/projects/FourierGPT/run_fft.py -i /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.sampled.gpt2xl.chop150.nll.txt -o /Users/xy/projects/FourierGPT/data/short/xsum_gpt-4.sampled.gpt2xl.chop150.nllzs.fftnorm.txt -p zscore --value norm\n"
     ]
    }
   ],
   "source": [
    "# Get spectrum for chopped data\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "est_name = 'gpt2xl'\n",
    "chop_k_values = [50,100,150]\n",
    "genre_list = ['xsum']\n",
    "source_list = ['original', 'sampled']\n",
    "\n",
    "data_root = os.path.abspath('../data/short/')\n",
    "script_root = os.path.abspath('../')\n",
    "print(f'abs path for data: {data_root}')\n",
    "print(f'abs path for script: {script_root}')\n",
    "\n",
    "for genre in genre_list:\n",
    "    for source in source_list:\n",
    "        for chop_k in chop_k_values:\n",
    "            input_filename = f'{genre}_gpt-4.{source}.{est_name}.chop{chop_k}.nll.txt'\n",
    "            input_path = os.path.join(data_root, input_filename)\n",
    "            output_filename = f'{genre}_gpt-4.{source}.{est_name}.chop{chop_k}.nllzs.fftnorm.txt'\n",
    "            output_path = os.path.join(data_root, output_filename)\n",
    "\n",
    "            if os.path.exists(input_path):\n",
    "                script_path = os.path.join(script_root, 'run_fft.py')\n",
    "                cmd = ['python', script_path, '-i', input_path, '-o', output_path, '-p', 'zscore', '--value', 'norm']\n",
    "                print(' '.join(cmd))\n",
    "                subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "require(\"data.table\")\n",
    "require(\"ggplot2\")\n",
    "\n",
    "orig_chop50 <- fread(\"../data/short/xsum_gpt-4.original.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "orig_chop50$Group <- \"Human\"\n",
    "samp_chop50 <- fread(\"../data/short/xsum_gpt-4.sampled.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "samp_chop50$Group <- \"Model\"\n",
    "\n",
    "orig_chop100 <- fread(\"../data/short/xsum_gpt-4.original.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "orig_chop100$Group <- \"Human\"\n",
    "samp_chop100 <- fread(\"../data/short/xsum_gpt-4.sampled.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "samp_chop100$Group <- \"Model\"\n",
    "\n",
    "orig_chop150 <- fread(\"../data/short/xsum_gpt-4.original.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "orig_chop150$Group <- \"Human\"\n",
    "samp_chop150 <- fread(\"../data/short/xsum_gpt-4.sampled.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "samp_chop150$Group <- \"Model\"\n",
    "\n",
    "orig_full <- fread(\"../data/gpt-4/xsum_gpt-4.original.gpt2xl.nllzs.fftnorm.txt\")\n",
    "orig_full$Group <- \"Human\"\n",
    "samp_full <- fread(\"../data/gpt-4/xsum_gpt-4.sampled.gpt2xl.nllzs.fftnorm.txt\")\n",
    "samp_full$Group <- \"Model\"\n",
    "\n",
    "d_chop50 <- rbind(samp_chop50, orig_chop50)\n",
    "d_chop50$ChopK <- \"50\"\n",
    "d_chop100 <- rbind(samp_chop100, orig_chop100)\n",
    "d_chop100$ChopK <- \"100\"\n",
    "d_chop150 <- rbind(samp_chop150, orig_chop150)\n",
    "d_chop150$ChopK <- \"150\"\n",
    "d_full <- rbind(samp_full, orig_full)\n",
    "d_full$ChopK <- \"Full\"\n",
    "d_chop <- rbind(d_chop50, d_chop100, d_chop150, d_full)\n",
    "d_chop$ChopK <- factor(d_chop$ChopK, levels=c(\"50\", \"100\", \"150\", \"Full\"))\n",
    "\n",
    "p <- ggplot(d_chop, aes(x=freq, y=power)) + \n",
    "    geom_smooth(aes(fill=Group, colour=Group, linetype=Group)) + \n",
    "    theme_bw() + theme(legend.position=c(.9,.2)) +\n",
    "    scale_color_brewer(palette=\"Set1\") + scale_fill_brewer(palette=\"Set1\") +\n",
    "    labs(x = bquote(omega[k]), y = bquote(X(omega[k]))) + \n",
    "    facet_wrap(~ChopK, ncol=4)\n",
    "ggsave(\"xsum_chop50_100_150.pdf\", plot=p, width=9, height=3)\n",
    "# plot(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
