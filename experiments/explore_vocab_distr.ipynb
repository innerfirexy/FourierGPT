{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from run_fft import FFTProcessor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "# Enable rpy2\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_processor = FFTProcessor(method='fft', \n",
    "                             preprocess='logzs', \n",
    "                             value='norm', \n",
    "                             require_sid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NLL data, apply log z-score\n",
    "# est_name = 'mistral'\n",
    "est_name = 'gpt2xl'\n",
    "\n",
    "nll_pubmed_orig = fft_processor._read_data(data_file=f'../data/gpt-4/pubmed_gpt-4.original.{est_name}.nll.txt')\n",
    "nll_pubmed_samp = fft_processor._read_data(data_file=f'../data/gpt-4/pubmed_gpt-4.sampled.{est_name}.nll.txt')\n",
    "\n",
    "# Log + Z-Score NLL\n",
    "fft_processor.preprocess = 'logzs'\n",
    "data = fft_processor._preprocess(nll_pubmed_orig)\n",
    "df_nlllogzs_orig = fft_processor._create_input_df(data)\n",
    "data = fft_processor._preprocess(nll_pubmed_samp)\n",
    "df_nlllogzs_samp = fft_processor._create_input_df(data)\n",
    "df_nlllogzs_orig['Source'] = 'Human'\n",
    "df_nlllogzs_samp['Source'] = 'Model'\n",
    "df_nlllogzs = pd.concat([df_nlllogzs_orig, df_nlllogzs_samp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing human lengths: 67.15333333333334 15.562234915189897\n",
      "writing model lengths: 69.04 15.420713342773738\n"
     ]
    }
   ],
   "source": [
    "# Explore the lengths of pubmed data\n",
    "print('writing human lengths:', \n",
    "      np.mean(list(map(len, nll_pubmed_orig))),\n",
    "      np.std(list(map(len, nll_pubmed_orig))))\n",
    "print('writing model lengths:',\n",
    "      np.mean(list(map(len, nll_pubmed_samp))),\n",
    "      np.std(list(map(len, nll_pubmed_samp))))\n",
    "\n",
    "# Need to examine the lengths of `Answer:` part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_nlllogzs\n",
    "require(\"data.table\")\n",
    "require(\"ggplot2\")\n",
    "\n",
    "dt <- data.table(df_nlllogzs)\n",
    "# nrow(dt)\n",
    "\n",
    "# Density plot\n",
    "# plot(density(dt[Source == \"Human\"]$value))\n",
    "\n",
    "vline.dat <- data.table(Source=c(\"Human\", \"Human\", \"Model\", \"Model\"), \n",
    "                        pos=c(\"left\", \"right\", \"left\", \"right\"),\n",
    "                        val=c(-1.667, -0.667, -1.5, -.5))\n",
    "p <- ggplot(dt, aes(x=value, fill=Source)) + geom_density(alpha=0.5) + theme_minimal() + \n",
    "    facet_wrap(~Source) +\n",
    "    geom_vline(vline.dat[Source==\"Human\" & pos==\"left\"], mapping=aes(xintercept=val), colour=\"red\", linetype=\"dashed\") + \n",
    "    geom_vline(vline.dat[Source==\"Human\" & pos==\"right\"], mapping=aes(xintercept=val), colour=\"red\", linetype=\"dashed\") + \n",
    "    geom_vline(vline.dat[Source==\"Model\" & pos==\"left\"], mapping=aes(xintercept=val), colour=\"green\", linetype=\"dashed\") + \n",
    "    geom_vline(vline.dat[Source==\"Model\" & pos==\"right\"], mapping=aes(xintercept=val), colour=\"green\", linetype=\"dashed\")\n",
    "plot(p)\n",
    "\n",
    "# Mistral results\n",
    "# print(nrow(dt[Source == \"Human\" & value > -1.5 & value <= -0.5]) / nrow(dt[Source == \"Human\"])) # 0.4091184, left peak\n",
    "# print(nrow(dt[Source == \"Human\" & value > -0.5 & value <= 2]) / nrow(dt[Source == \"Human\"])) # 0.5674812, right plateau\n",
    "\n",
    "# GPT2-xl results\n",
    "print(nrow(dt[Source == \"Human\" & value > -1.667 & value <= -0.667]) / nrow(dt[Source == \"Human\"])) #\n",
    "print(nrow(dt[Source == \"Human\" & value > -0.667 & value <= 1.5]) / nrow(dt[Source == \"Human\"])) #\n",
    "print(nrow(dt[Source == \"Model\" & value > -1.5 & value <= -0.5]) / nrow(dt[Source == \"Model\"])) #\n",
    "print(nrow(dt[Source == \"Model\" & value > -0.5 & value <= 1.5]) / nrow(dt[Source == \"Model\"])) #\n",
    "\n",
    "\n",
    "summary(dt[Source == \"Human\"]$value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to see if tokenids.txt and nll.txt are aligned\n",
    "tokenids_orig = fft_processor._read_data(data_file=f'../data/gpt-4/pubmed_gpt-4.original.gpt2_tokenids.txt')\n",
    "tokenids_samp = fft_processor._read_data(data_file=f'../data/gpt-4/pubmed_gpt-4.sampled.gpt2_tokenids.txt')\n",
    "# convert ids to int\n",
    "tokenids_orig = [[int(x) for x in ids] for ids in tokenids_orig]\n",
    "tokenids_samp = [[int(x) for x in ids] for ids in tokenids_samp]\n",
    "\n",
    "for i in range(len(tokenids_orig)):\n",
    "    assert len(tokenids_orig[i]) == len(nll_pubmed_orig[i]) + 1\n",
    "for i in range(len(tokenids_samp)):\n",
    "    assert len(tokenids_samp[i]) == len(nll_pubmed_samp[i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2931, 3.2406, 5.9268, 8.665, 7.573, 1.333, 6.0375, 4.6368, 1.2882, 6.3584]\n"
     ]
    }
   ],
   "source": [
    "# Get log z-score transformed NLL \n",
    "fft_processor.preprocess = 'logzs'\n",
    "nlllogzs_orig = fft_processor._preprocess(nll_pubmed_orig)\n",
    "nlllogzs_samp = fft_processor._preprocess(nll_pubmed_samp)\n",
    "print(nll_pubmed_orig[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988\n",
      "2121\n",
      "left top 10 tokens:  [(286, 205), (25, 142), (13, 119), (30, 118), (284, 111), (262, 75), (11, 59), (351, 56), (287, 52), (12, 46)]\n",
      "right top 10 tokens:  [(25, 183), (262, 177), (287, 176), (23998, 149), (290, 137), (257, 113), (13, 112), (11, 76), (318, 72), (286, 69)]\n"
     ]
    }
   ],
   "source": [
    "left = -1.667\n",
    "mid = -0.667\n",
    "right = 1.5\n",
    "\n",
    "# Vocabulary of human, in left peak and right plateau, respectively\n",
    "vocab_human_left = Counter()\n",
    "vocab_human_right = Counter()\n",
    "for i in range(len(nlllogzs_orig)):\n",
    "    token_ids = tokenids_orig[i]\n",
    "    for j in range(len(nlllogzs_orig[i])):\n",
    "        val = nlllogzs_orig[i][j]\n",
    "        if val > left and val <= mid:\n",
    "            vocab_human_left[token_ids[j+1]] += 1\n",
    "        elif val > mid and val <= right:\n",
    "            vocab_human_right[token_ids[j+1]] += 1\n",
    "\n",
    "print(len(vocab_human_left))\n",
    "print(len(vocab_human_right))\n",
    "\n",
    "print('left top 10 tokens: ', vocab_human_left.most_common(10))\n",
    "print('right top 10 tokens: ', vocab_human_right.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the token ids\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('/Users/xy/models/gpt2-xl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left top tokens: \n",
      " of : . ?  to  the ,  with  in -  that  and  be  for  a  is al  patients  by ) right top tokens: \n",
      ":  the  in  Answer  and  a . ,  is  of  for  to -  The  patients  may  with  Does  Is  are "
     ]
    }
   ],
   "source": [
    "print('left top tokens: ')\n",
    "for token_id, token_count in vocab_human_left.most_common(20):\n",
    "    token = gpt2_tokenizer.decode(token_id)\n",
    "    print(f'{token}', end=' ')\n",
    "\n",
    "print('right top tokens: ')\n",
    "for token_id, token_count in vocab_human_right.most_common(20):\n",
    "    token = gpt2_tokenizer.decode(token_id)\n",
    "    print(f'{token}', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the lengths of Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing human lengths: 203.25333333333333 19.312581966744432\n",
      "writing model lengths: 207.29333333333332 15.295989307295194\n"
     ]
    }
   ],
   "source": [
    "est_name = 'gpt2xl'\n",
    "\n",
    "nll_xsum_orig = fft_processor._read_data(data_file=f'../data/gpt-4/writing_gpt-4.original.{est_name}.nll.txt')\n",
    "nll_xsum_samp = fft_processor._read_data(data_file=f'../data/gpt-4/writing_gpt-4.sampled.{est_name}.nll.txt')\n",
    "\n",
    "print('writing human lengths:', \n",
    "      np.mean(list(map(len, nll_xsum_orig))),\n",
    "      np.std(list(map(len, nll_xsum_orig))))\n",
    "print('writing model lengths:',\n",
    "      np.mean(list(map(len, nll_xsum_samp))),\n",
    "      np.std(list(map(len, nll_xsum_samp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nlls(nlls, output_file):\n",
    "    import torch\n",
    "    with open(output_file, 'w') as f:\n",
    "        for res in nlls:\n",
    "            if isinstance(res, torch.Tensor):\n",
    "                res = res.numpy().tolist()\n",
    "            res_str = ' '.join(f'{num:.4f}' for num in res)\n",
    "            f.write(f'{res_str}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop the first k=50, 100, 150 tokens\n",
    "est_name = 'gpt2xl'\n",
    "chop_k = 150\n",
    "\n",
    "nll_xsum_orig_chop = [nll[:chop_k] for nll in nll_xsum_orig]\n",
    "nll_xsum_samp_chop = [nll[:chop_k] for nll in nll_xsum_samp]\n",
    "\n",
    "write_nlls(nll_xsum_orig_chop, f'../data/short/writing_gpt-4.original.{est_name}.chop{chop_k}.nll.txt')\n",
    "write_nlls(nll_xsum_samp_chop, f'../data/short/writing_gpt-4.sampled.{est_name}.chop{chop_k}.nll.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "require(\"data.table\")\n",
    "require(\"ggplot2\")\n",
    "\n",
    "orig_chop50 <- fread(\"../data/short/writing_gpt-4.original.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "orig_chop50$Group <- \"Human\"\n",
    "samp_chop50 <- fread(\"../data/short/writing_gpt-4.sampled.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "samp_chop50$Group <- \"Model\"\n",
    "\n",
    "orig_chop100 <- fread(\"../data/short/writing_gpt-4.original.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "orig_chop100$Group <- \"Human\"\n",
    "samp_chop100 <- fread(\"../data/short/writing_gpt-4.sampled.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "samp_chop100$Group <- \"Model\"\n",
    "\n",
    "orig_chop150 <- fread(\"../data/short/writing_gpt-4.original.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "orig_chop150$Group <- \"Human\"\n",
    "samp_chop150 <- fread(\"../data/short/writing_gpt-4.sampled.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "samp_chop150$Group <- \"Model\"\n",
    "\n",
    "orig_full <- fread(\"../data/gpt-4/writing_gpt-4.original.gpt2xl.nllzs.fftnorm.txt\")\n",
    "orig_full$Group <- \"Human\"\n",
    "samp_full <- fread(\"../data/gpt-4/writing_gpt-4.sampled.gpt2xl.nllzs.fftnorm.txt\")\n",
    "samp_full$Group <- \"Model\"\n",
    "\n",
    "d_chop50 <- rbind(samp_chop50, orig_chop50)\n",
    "d_chop50$ChopK <- \"50\"\n",
    "d_chop100 <- rbind(samp_chop100, orig_chop100)\n",
    "d_chop100$ChopK <- \"100\"\n",
    "d_chop150 <- rbind(samp_chop150, orig_chop150)\n",
    "d_chop150$ChopK <- \"150\"\n",
    "d_full <- rbind(samp_full, orig_full)\n",
    "d_full$ChopK <- \"Full\"\n",
    "d_chop <- rbind(d_chop50, d_chop100, d_chop150, d_full)\n",
    "d_chop$ChopK <- factor(d_chop$ChopK, levels=c(\"50\", \"100\", \"150\", \"Full\"))\n",
    "\n",
    "p <- ggplot(d_chop, aes(x=freq, y=power)) + \n",
    "    geom_smooth(aes(fill=Group, colour=Group, linetype=Group)) + \n",
    "    theme_bw() + theme(legend.position=c(.9,.2)) +\n",
    "    scale_color_brewer(palette=\"Set1\") + scale_fill_brewer(palette=\"Set1\") +\n",
    "    labs(x = bquote(omega[k]), y = bquote(X(omega[k]))) + \n",
    "    facet_wrap(~ChopK, ncol=4)\n",
    "ggsave(\"writing_chop50_100_150.pdf\", plot=p, width=12, height=4)\n",
    "# plot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chop xsum by lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xsum human lengths: 210.03333333333333 18.089929672487827\n",
      "xsum model lengths: 205.53333333333333 14.614908674211945\n"
     ]
    }
   ],
   "source": [
    "est_name = 'gpt2xl'\n",
    "\n",
    "nll_xsum_orig = fft_processor._read_data(data_file=f'../data/gpt-4/xsum_gpt-4.original.{est_name}.nll.txt')\n",
    "nll_xsum_samp = fft_processor._read_data(data_file=f'../data/gpt-4/xsum_gpt-4.sampled.{est_name}.nll.txt')\n",
    "\n",
    "print('xsum human lengths:', \n",
    "      np.mean(list(map(len, nll_xsum_orig))),\n",
    "      np.std(list(map(len, nll_xsum_orig))))\n",
    "print('xsum model lengths:',\n",
    "      np.mean(list(map(len, nll_xsum_samp))),\n",
    "      np.std(list(map(len, nll_xsum_samp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop the first k=50, 100, 150 tokens\n",
    "est_name = 'gpt2xl'\n",
    "chop_k = 100\n",
    "\n",
    "nll_xsum_orig_chop = [nll[:chop_k] for nll in nll_xsum_orig]\n",
    "nll_xsum_samp_chop = [nll[:chop_k] for nll in nll_xsum_samp]\n",
    "\n",
    "write_nlls(nll_xsum_orig_chop, f'../data/short/xsum_gpt-4.original.{est_name}.chop{chop_k}.nll.txt')\n",
    "write_nlls(nll_xsum_samp_chop, f'../data/short/xsum_gpt-4.sampled.{est_name}.chop{chop_k}.nll.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "require(\"data.table\")\n",
    "require(\"ggplot2\")\n",
    "\n",
    "orig_chop50 <- fread(\"../data/short/xsum_gpt-4.original.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "orig_chop50$Group <- \"Human\"\n",
    "samp_chop50 <- fread(\"../data/short/xsum_gpt-4.sampled.gpt2xl.chop50.nllzs.fftnorm.txt\")\n",
    "samp_chop50$Group <- \"Model\"\n",
    "\n",
    "orig_chop100 <- fread(\"../data/short/xsum_gpt-4.original.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "orig_chop100$Group <- \"Human\"\n",
    "samp_chop100 <- fread(\"../data/short/xsum_gpt-4.sampled.gpt2xl.chop100.nllzs.fftnorm.txt\")\n",
    "samp_chop100$Group <- \"Model\"\n",
    "\n",
    "orig_chop150 <- fread(\"../data/short/xsum_gpt-4.original.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "orig_chop150$Group <- \"Human\"\n",
    "samp_chop150 <- fread(\"../data/short/xsum_gpt-4.sampled.gpt2xl.chop150.nllzs.fftnorm.txt\")\n",
    "samp_chop150$Group <- \"Model\"\n",
    "\n",
    "orig_full <- fread(\"../data/gpt-4/xsum_gpt-4.original.gpt2xl.nllzs.fftnorm.txt\")\n",
    "orig_full$Group <- \"Human\"\n",
    "samp_full <- fread(\"../data/gpt-4/xsum_gpt-4.sampled.gpt2xl.nllzs.fftnorm.txt\")\n",
    "samp_full$Group <- \"Model\"\n",
    "\n",
    "d_chop50 <- rbind(samp_chop50, orig_chop50)\n",
    "d_chop50$ChopK <- \"50\"\n",
    "d_chop100 <- rbind(samp_chop100, orig_chop100)\n",
    "d_chop100$ChopK <- \"100\"\n",
    "d_chop150 <- rbind(samp_chop150, orig_chop150)\n",
    "d_chop150$ChopK <- \"150\"\n",
    "d_full <- rbind(samp_full, orig_full)\n",
    "d_full$ChopK <- \"Full\"\n",
    "d_chop <- rbind(d_chop50, d_chop100, d_chop150, d_full)\n",
    "d_chop$ChopK <- factor(d_chop$ChopK, levels=c(\"50\", \"100\", \"150\", \"Full\"))\n",
    "\n",
    "p <- ggplot(d_chop, aes(x=freq, y=power)) + \n",
    "    geom_smooth(aes(fill=Group, colour=Group, linetype=Group)) + \n",
    "    theme_bw() + theme(legend.position=c(.9,.2)) +\n",
    "    scale_color_brewer(palette=\"Set1\") + scale_fill_brewer(palette=\"Set1\") +\n",
    "    labs(x = bquote(omega[k]), y = bquote(X(omega[k]))) + \n",
    "    facet_wrap(~ChopK, ncol=4)\n",
    "ggsave(\"xsum_chop50_100_150.pdf\", plot=p, width=12, height=4)\n",
    "# plot(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
