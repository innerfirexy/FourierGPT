{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the raw output data/gpt-4/pubmed_*.nll.txt, each line contains N-1 negative log-probabilities for a sentence \n",
    "# of length N (tokens)\n",
    "# The specialty of pubmed data is that we want to examine the *answer* part, i.e., the tokens after \"Answer:\" in the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 15:14:17,864 - modelscope - INFO - PyTorch version 2.2.0 Found.\n",
      "2024-05-07 15:14:17,864 - modelscope - INFO - Loading ast index from /Users/xy/.cache/modelscope/ast_indexer\n",
      "2024-05-07 15:14:17,929 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 b6a37aa50898b7ca29cb870cc35ad7a7 and a total number of 976 components indexed\n"
     ]
    }
   ],
   "source": [
    "# import from parent directory\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from model import Model\n",
    "from modelscope import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read nll data\n",
    "def _read_data(data_file, N=np.inf):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            num = list(map(float, line.split()))\n",
    "            data.append(num)\n",
    "            count += 1\n",
    "            if count >= N:\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.175, 0.0119, 3.3275, 10.8108, 7.6979]\n",
      "[5.2005, 0.0121, 3.291, 10.8107, 7.6796]\n"
     ]
    }
   ],
   "source": [
    "nll_orig = _read_data('../data/gpt-4/pubmed_gpt-4.original.mistral.nll.txt')\n",
    "print(nll_orig[0][:5])\n",
    "\n",
    "nll_samp = _read_data('../data/gpt-4/pubmed_gpt-4.sampled.mistral.nll.txt')\n",
    "print(nll_samp[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw text data\n",
    "def _read_raw_text(data_file, N=np.inf):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            data.append(line)\n",
    "            count += 1\n",
    "            if count >= N:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def _write_raw_text(data, data_file):\n",
    "    with open(data_file, 'w') as f:\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is an adva\n",
      "Question: Is an adva\n"
     ]
    }
   ],
   "source": [
    "text_orig = _read_raw_text('../data/gpt-4/pubmed_gpt-4.original.txt')\n",
    "print(text_orig[0][:20])\n",
    "text_samp = _read_raw_text('../data/gpt-4/pubmed_gpt-4.sampled.txt')\n",
    "print(text_samp[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the tokenized result for a line of text\n",
    "model_dir = \"/Users/xy/models/mistral-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is an advance care planning model feasible in community palliative care? Answer: An advance care planning model is feasible for community palliative care services. Quality audit processes are an essential component of the Model with documentation of advance care planning discussion established as an important outcome measure.\n",
      "tensor([[    1, 22478, 28747,  1691,   396,  8670,  1656,  7394,  2229, 25953,\n",
      "          1070,   297,  3618,   284,   455, 26938,  1656, 28804, 26307, 28747,\n",
      "          1094,  8670,  1656,  7394,  2229,   349, 25953,  1070,   354,  3618,\n",
      "           284,   455, 26938,  1656,  3345, 28723, 20612, 24790,  9537,   460,\n",
      "           396,  7974,  5641,   302,   272,  8871,   395, 12905,   302,  8670,\n",
      "          1656,  7394,  8387,  6740,   390,   396,  2278, 14120,  5266, 28723]])\n",
      "18 Answer\n"
     ]
    }
   ],
   "source": [
    "print(text_orig[0])\n",
    "input_ids = tokenizer(text_orig[0], return_tensors='pt')['input_ids']\n",
    "print(input_ids)\n",
    "\n",
    "for i in range(input_ids.shape[1]):\n",
    "    decoded_token = tokenizer.decode(input_ids[0, i].item())\n",
    "    if decoded_token == 'Answer':\n",
    "        print(i, decoded_token)\n",
    "\n",
    "# First decoded token is \"<s>\" at each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 60\n",
      "70 71\n",
      "87 88\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check per-line length in nll_orig and text_orig\n",
    "for i in range(3):\n",
    "    input_ids = tokenizer(text_orig[i], return_tensors='pt')['input_ids']\n",
    "    print(len(nll_orig[i]), input_ids.shape[1])\n",
    "\n",
    "incorrect_count = 0\n",
    "for i in range(len(nll_orig)):\n",
    "    input_ids = tokenizer(text_orig[i], return_tensors='pt')['input_ids']\n",
    "    if len(nll_orig[i]) != input_ids.shape[1] - 1:\n",
    "        incorrect_count += 1\n",
    "print(incorrect_count)\n",
    "# We have verified that for all lines\n",
    "# len(nll_orig[i]) == input_ids.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "def extract_answer_nll(nlls, text):\n",
    "    new_nlls = []\n",
    "    for i in range(len(nlls)):\n",
    "        input_ids = tokenizer(text[i], return_tensors='pt')['input_ids']\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            decoded_token = tokenizer.decode(input_ids[0, j].item())\n",
    "            if decoded_token == 'Answer':\n",
    "                # skip \"Answer\" and \":\"\n",
    "                next_token = tokenizer.decode(input_ids[0, j+1].item())\n",
    "                assert next_token == ':'\n",
    "                new_nlls.append(nlls[i][j+1:])\n",
    "                break\n",
    "    return new_nlls\n",
    "\n",
    "new_nlls_orig = extract_answer_nll(nll_orig, text_orig)\n",
    "print(len(new_nlls_orig))\n",
    "new_nlls_samp = extract_answer_nll(nll_samp, text_samp)\n",
    "print(len(new_nlls_samp)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "def write_nlls(nlls, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for res in nlls:\n",
    "            if isinstance(res, torch.Tensor):\n",
    "                res = res.numpy().tolist()\n",
    "            res_str = ' '.join(f'{num:.4f}' for num in res)\n",
    "            f.write(f'{res_str}\\n')\n",
    "\n",
    "write_nlls(new_nlls_orig, '../data/gpt-4/pubmed_AnsInCtx_gpt-4.original.mistral.nll.txt')\n",
    "write_nlls(new_nlls_samp, '../data/gpt-4/pubmed_AnsInCtx_gpt-4.sampled.mistral.nll.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pure text in pubmed after \"Answer:\" and store in individual files\n",
    "def extract_answer_text(text: list[str]) -> list[str]:\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        parts = line.split('Answer:')\n",
    "        assert len(parts) == 2\n",
    "        new_text.append(parts[1].strip())\n",
    "    return new_text\n",
    "\n",
    "ans_text_orig = extract_answer_text(text_orig)\n",
    "ans_text_samp = extract_answer_text(text_samp)\n",
    "\n",
    "_write_raw_text(ans_text_orig, '../data/gpt-4/pubmed_Ans_gpt-4.original.txt')\n",
    "_write_raw_text(ans_text_samp, '../data/gpt-4/pubmed_Ans_gpt-4.sampled.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
