{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the raw output data/gpt-4/pubmed_*.nll.txt, each line contains N-1 negative log-probabilities for a sentence \n",
    "# of length N (tokens)\n",
    "# The specialty of pubmed data is that we want to examine the *answer* part, i.e., the tokens after \"Answer:\" in the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 16:40:31,510 - modelscope - INFO - PyTorch version 2.2.0 Found.\n",
      "2024-06-01 16:40:31,511 - modelscope - INFO - Loading ast index from /Users/xy/.cache/modelscope/ast_indexer\n",
      "2024-06-01 16:40:31,580 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 b6a37aa50898b7ca29cb870cc35ad7a7 and a total number of 976 components indexed\n"
     ]
    }
   ],
   "source": [
    "# import from parent directory\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from model import Model\n",
    "from modelscope import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read nll data\n",
    "def _read_data(data_file, N=np.inf):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            num = list(map(float, line.split()))\n",
    "            data.append(num)\n",
    "            count += 1\n",
    "            if count >= N:\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_orig = _read_data('../data/gpt-4/pubmed_gpt-4.original.mistral.nll.txt')\n",
    "# print(nll_orig[0][:5])\n",
    "\n",
    "nll_samp = _read_data('../data/gpt-4/pubmed_gpt-4.sampled.mistral.nll.txt')\n",
    "# print(nll_samp[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw text data\n",
    "def _read_raw_text(data_file, N=np.inf):\n",
    "    data = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            data.append(line)\n",
    "            count += 1\n",
    "            if count >= N:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def _write_raw_text(data, data_file):\n",
    "    with open(data_file, 'w') as f:\n",
    "        for line in data:\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is an adva\n",
      "Question: Is an adva\n"
     ]
    }
   ],
   "source": [
    "text_orig = _read_raw_text('../data/gpt-4/pubmed_gpt-4.original.txt')\n",
    "print(text_orig[0][:20])\n",
    "text_samp = _read_raw_text('../data/gpt-4/pubmed_gpt-4.sampled.txt')\n",
    "print(text_samp[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_name = 'gpt2xl' # or mistral etc.\n",
    "\n",
    "# Load tokenizer accordingly\n",
    "if est_name == 'mistral':\n",
    "    model_dir = \"/Users/xy/models/mistral-7b\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "elif est_name == 'gpt2xl':\n",
    "    model_dir = \"/Users/xy/models/gpt2-xl\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Read NLL data\n",
    "nll_orig = _read_data(f'../data/gpt-4/pubmed_gpt-4.original.{est_name}.nll.txt')\n",
    "nll_samp = _read_data(f'../data/gpt-4/pubmed_gpt-4.sampled.{est_name}.nll.txt')\n",
    "\n",
    "def extract_answer_nll(nlls, text):\n",
    "    new_nlls = []\n",
    "    for i in range(len(nlls)):\n",
    "        input_ids = tokenizer(text[i], return_tensors='pt')['input_ids']\n",
    "        for j in range(input_ids.shape[1]):\n",
    "            decoded_token = tokenizer.decode(input_ids[0, j].item())\n",
    "            if decoded_token == 'Answer':\n",
    "                # skip \"Answer\" and \":\"\n",
    "                next_token = tokenizer.decode(input_ids[0, j+1].item())\n",
    "                assert next_token == ':'\n",
    "                new_nlls.append(nlls[i][j+1:])\n",
    "                break\n",
    "    return new_nlls\n",
    "\n",
    "# Extract Answer NLLs\n",
    "new_nlls_orig = extract_answer_nll(nll_orig, text_orig)\n",
    "new_nlls_samp = extract_answer_nll(nll_samp, text_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "def write_nlls(nlls, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for res in nlls:\n",
    "            if isinstance(res, torch.Tensor):\n",
    "                res = res.numpy().tolist()\n",
    "            res_str = ' '.join(f'{num:.4f}' for num in res)\n",
    "            f.write(f'{res_str}\\n')\n",
    "\n",
    "write_nlls(new_nlls_orig, f'../data/gpt-4/pubmed_AnsInCtx_gpt-4.original.{est_name}.nll.txt')\n",
    "write_nlls(new_nlls_samp, f'../data/gpt-4/pubmed_AnsInCtx_gpt-4.sampled.{est_name}.nll.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pure text in pubmed after \"Answer:\" and store in individual files\n",
    "def extract_answer_text(text: list[str]) -> list[str]:\n",
    "    new_text = []\n",
    "    for line in text:\n",
    "        parts = line.split('Answer:')\n",
    "        assert len(parts) == 2\n",
    "        new_text.append(parts[1].strip())\n",
    "    return new_text\n",
    "\n",
    "ans_text_orig = extract_answer_text(text_orig)\n",
    "ans_text_samp = extract_answer_text(text_samp)\n",
    "\n",
    "_write_raw_text(ans_text_orig, '../data/gpt-4/pubmed_Ans_gpt-4.original.txt')\n",
    "_write_raw_text(ans_text_samp, '../data/gpt-4/pubmed_Ans_gpt-4.sampled.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
